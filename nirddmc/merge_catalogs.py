import typer
import pandas as pd
import intake
from typing import List
import intake_esm
import pathlib
import datetime
from intake_esm.cat import Aggregation

def _check_column_names_match(dfs: List[pd.DataFrame], filepaths: List[str]) -> None:
    for i in range(1, len(dfs)):
        if not dfs[i].columns.equals(dfs[0].columns):
            raise ValueError(f"Column names do not match between {filepaths[0]} and {filepaths[i]}")


def _join_tables_from_filepath(filepaths: List[str]):
    if filepaths[0].endswith('.csv'):
        dfs = [pd.read_csv(filepath) for filepath in filepaths]
    elif filepaths[0].endswith('.json'):
        dfs = [intake.open_esm_datastore(filepath).df for filepath in filepaths]
    _check_column_names_match(dfs, filepaths)
    df = pd.concat(dfs, ignore_index=True)
    df['version']=df['version'].str.slice(start=1).astype(int)
    return df

def _join_tables_from_intake_urls(urls: List[str]):
    dfs = [intake.open_esm_datastore(url).df for url in urls]
    df = pd.concat(dfs, ignore_index=True)
    return df


def merge_cmip_catalogs(catalogs: List[str] = typer.Option(..., "--catalogs","-c", help='List of catalogs to merge'),
        name: str = typer.Option('merge_cmip','--file-name','-fn', help='what to name the merged catalog'),
        directory: str = typer.Option('./', "--dir", "--directory", help='Where to store created intake catalog'),
        compression: bool = typer.Option(False, help='Whether to compress the output csv file'),
        url_remote_tables: List[str] = typer.Option(None, '--url-remote','--url', help='List of url to intake cataloges from remote sources to join with local tables, e.g. pangeo: https://storage.googleapis.com/cmip6/pangeo-cmip6.json'),
        description: str = typer.Option(None, help='Detailed multi-line description to fully explain the collection')):
    combined_local_table = _join_tables_from_filepath(catalogs)


    if url_remote_tables:
        remote_table = _join_tables_from_intake_urls(url_remote_tables)
        # remove entries in the remote table that are already in the local table
        keys = [key 
            for key in combined_local_table.columns.values 
            if key in remote_table.columns.values
            ]
        i1 = combined_local_table.set_index(keys).index
        i2 = remote_table.set_index(keys).index

        remote_filtered = remote_table[~i2.isin(i1)]
        merged_tab = pd.concat([combined_local_table, remote_filtered], ignore_index=True)
        merged_tab.loc[pd.isna(merged_tab["path"]),"format"]="zarr"
        merged_tab.loc[pd.isna(merged_tab["time_range"]),"time_range"]="*"

        merged_tab.loc[pd.isna(merged_tab["path"]),"path"]=merged_tab["zstore"]

        del merged_tab["zstore"]

    else:
        merged_tab = combined_local_table
    
        merged_tab["format"]="netcdf"


    last_update = datetime.datetime.now().strftime("%Y-%m-%d")
    merged_tab = merged_tab.drop_duplicates(subset=["path","version"])
    merge_catalogs={"esmcat_version":"0.1.0"}
    if description is None:
        description = f"Catalog generated by merging {catalogs} and {url_remote_tables}"

    merge_catalogs["description"]=description


    attributes=[]
    directory_structure_template="mip_era/activity_id/institution_id/source_id/experiment_id/member_id/table_id/variable_id/grid_label/version"

    for att in directory_structure_template.split('/'):
        attributes.append(
            dict(column_name=att,
                vocabulary=f"https://raw.githubusercontent.com/WCRP-CMIP/CMIP6_CVs/master/CMIP6_{att}.json"
                )
        )
    merge_catalogs["attributes"]=attributes  

    assets={
        "column_name": "path",
        "format_column_name" : "format"
    }
    merge_catalogs["assets"]=assets

    aggregation_control=dict(
    variable_column_name="variable_id",
    groupby_attrs=[
        "activity_id",
        "institution_id"
    ]
    )

    aggregation_control["aggregations"]=[
        Aggregation(
        attribute_name="variable_id",
        type="union"
    )]

    aggregation_control["aggregations"].append(
        Aggregation(
            attribute_name="time_range",
            type="join_existing",
            options={ "dim": "time", "coords": "minimal", "compat": "override" }
        )
    )

    aggregation_control["aggregations"].append(
        Aggregation(
            attribute_name= "member_id",
            type= "join_new",
            options={ "coords": "minimal", "compat": "override" }
        )
    )
    merge_catalogs["last_updated"]=last_update
    merge_catalogs["aggregation_control"]=aggregation_control
    esm_merged = intake.open_esm_datastore({'esmcat':merge_catalogs, 'df': merged_tab})


    esm_merged.serialize(name=name, directory=directory, catalog_type="file")
